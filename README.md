**Class Times**:	Fridays 14:00 - 15:30 (Sometimes Wednesdays 11:30 - 13:00 see [syllabus](#syllabus)). <br>
**Location**:	Ground floor lecture theater, Gatsby Computational Neuroscience Unit, Sainsbury Wellcome Centre, [25 Howland Street](https://goo.gl/maps/ew5v5F6F7bF2). <br>
**Instructor**:	Carlo Ciliberto <br>
**TAs**: Stephen Pasteris <br>
**Office Hours**:	Thursdays 14:00 - 15:00. 3rd floor Hub, [66 Gower street](https://goo.gl/maps/n1hb1BV2erR2)<br>
**Email Contact** :	cciliber (a) gmail.com, stephen.pasteris (a) gmail.com <br>
**News!** The assignment is out! Check it out on [Moodle](https://moodle.ucl.ac.uk). **Due Date: 5 Jan 2018**. 

This course represents half of [Advanced Topics in Machine Learning](http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi13_advanced_topics_in_machine_learning/) (aka COMP GI13 / COMP M050) from the [UCL CS MSc on Machine Learning](http://www.cs.ucl.ac.uk/prospective_students/msc_machine_learning/). 

The other half is [Reproducing kernel Hilbert spaces in Machine Learning](http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/rkhscourse.html) (Taught by Prof. [Arthur Gretton](http://www.gatsby.ucl.ac.uk/~gretton/)).

Course announcements will be posted on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/csml-advanced-topics).

## Course Description

Statistical Learning Theory (SLT) studies the problem of learning from empirical observations (data) to predict and/or understand the behavior of an unknown phenomenon (e.g. the dynamics of the stock market or the activations patterns in the human brain). SLT provides a mathematical framework within which it is
possible rigorously address questions such as "How to design a learning algorithm", "what does it mean for an algorithm to 'solve' a learning problem" or "How to compare two learning algorithms".  

The goal of this course is to introduce students to the ideas behind most well-established learning algorithms and provide fundamental insights on how to use them in practice or to design new ones. 


## Prerequisites

Linear Algebra, Probability Theory, Calculus.

## Grading

Final grades will depend on *one* project assignment (50%) and a final exam (50%). 
The assignment can be accessed from [Moodle](https://moodle.ucl.ac.uk). **Due Date: 5 Jan 2018**. 

## Syllabus

**Class** | **Date** | **Topic**
 :---: | :--- | :---
1 | Fri Oct 06 | [Course Overview](/intro-slt/slides/lec1.pdf)
2 | Fri Oct 13 | [Overfitting and Regularization I](/intro-slt/slides/lec2-3.pdf)
3 | Wed Oct 18 | [Overfitting and Regularization II](/intro-slt/slides/lec2-3.pdf)
4 | Fri Oct 20 | [Tikhonov Regularization](/intro-slt/slides/lec4.pdf)
5 | Fri Nov 03 | [Generalization Error and Stability](/intro-slt/slides/lec5.pdf)
6 | Fri Nov 10 | Computational Regularization via Early Stopping I
7 | Wed Nov 22 | Computational Regularization via Early Stopping II
8 | Fri Nov 24 | Large Scale Learning and Random Projection Methods
9 | Wed Nov 29 | Structured Prediction I
10 | Fri Dec 15| Structured Prediction II

## Reading List

There is no required text for the course. Below is a number of useful references:

- O. Bousquet, S. Boucheron and G. Lugosi [Introduction to Statistical Learning Theory (Tutorial)](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2819.pdf).
- T. Poggio and L. Rosasco course [slides and videos](http://www.mit.edu/~9.520).
- P. Liang course [notes](https://web.stanford.edu/class/cs229t/Lectures/percy-notes.pdf).
- N. Cristianini and J. Shawe-Taylor. Kernel Methods for Pattern Analysis . Cambridge University Press, 2004.
- I. Steinwart and A. Christmann. [Support Vector Machines](http://www.staff.uni-bayreuth.de/~bt230781/svm.html) Springer, 2008.
- S. Shalev-Shwartz and S. Ben-David [Understanding Machine Learning: From Theory to Algorithms (Online Book)](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html). Cambridge University Press , 2014.



